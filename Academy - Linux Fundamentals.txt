prefix man for manual. --h or -help is a short form for it.

Each manual has a short description available within it.
The command "apropos <keyword>" returns a succinct summary of the command.

System information
The following are useful system commands:
whoami => displays current username
uname => displays basic information about OS and system
ifconfig => assign or view addresses to a network interface and or configure its parameters
ip => utility to show routing, network devices, interfaces and tunnels. Also used for changing its settings.
netstat => shows network settings
ps => shows process status
who => displays who is logged in
env => displays environment (variables)
lsblk => displays blocked devices
lsusb => displays USB devices
lsof => displays open files
lspci => displays PCI devices
hostname => prints the name of the computer we are logged in to.
Differs from whoami which shows the username; not the name of the computer.

id
This command builds on top of the whoami command and prints out the effective group membership and IDs. Useful to probe for more information regarding groups and identities in them.
There are some existing built-in groups:
adm: admin group: can read logs in var/logs
sudo: all-powerful root user: can run anything. Can be used for escalating privileges
for other users.

uname can be used to detect system information regarding the machine; then from there one can scan for vulnerabilities on the machine.


Logging in via SSH:
SSH is a protocol that allows clients to access and execute commands on a remote computer.
Typically used for Linux-based hosts and or Unix-like OS.
SSH login: ssh [username]@[IP address]

mail spool:
a location where the computer temporarily stores incoming and outgoing email messages before processed by the mail application
In Linux, this location is typically: /var/spool/main/$USER

/var directory contains files that are expected to change in size or content as the system operates.

The env command in Linux can provide a lot of information - environment variables and values set on the computer.

MTU: Maximum Transmission Unit. Largest size of a data packet that can be transmitted over a network connection. Size of payload portion.
The downsides of having larger packets: Higher chance of packet fragmentation which will have more overhead.

Navigation
ls vs dir: dir is used in Windows command prompt; ls is used in Unix-like systems
ls -l provides more information : long format
ls -a lists all files of a directory. + hidden files
ls -i lists in the inode numbers of each file or directory

inode is a data structure used by UNIX-like systems to store metadata regarding a file or directory in a system.
It contains information about a file; and also within it a pointer to its actual address (data blocks) on the physical disk where the file contents are stored.

After cd, you can call cd - to return back to previous directory

Shortcut: you can call like cd D
Then the D represents some directory. From here, you can press tab to auto-complete if there is only one directory that starts with D; else twice to display all directories starting with that letter.

rmdir to remove directories; rm to remove files

One dot (.) represents current directory; two dots (..) represents parent directory
clear clears the terminal



File and directory manipulation
mkdir makes a directory; touch creates a file
mkdir -p /directory/path
-p with mkdir allows you to create a directory at a given path.

tree .
This will print out a nicely-formatted tree-like structure of your directory.
Root is your current directory.

mv <from> <to>
This command is not just for moving files; it can be used for renaming as well.

cp <from> <to>
This command copy-pastes the directory from and to the designated path.


ls -lt will also sort based on time (-t parameter)





Editing files
We use a text editor.
Nano editor is used in this example:
nano notes.txt creates a new file called notes.txt

In the editor:
The caret (^) stands for our "[CTRL]" key.

The exit functionality will save the file

To view contents of a file: cat <file>

Vim
Open-source text editor.
Has 5 modes:
1. Normal
    a. All inputs are treated as editor commands. We start here
2. Insert
    b. All inputs are inserted.
3. Visual
    c. Mark parts of text. These marked texts then can be manipulated - replaced; deleted; copied...
4. Command
    d. All inputs are treated as single-line commands at bottom of editor. Commands are like sorting; replacing sections; deleting them...
5. Replace
    e. All inputs will replace existing once. If no existing character at position to replace, then it creates the new input.

vimtutor is an excellent way to start using Vim.



Finding files and directories:
which <executable program>: this returns the file path of executable program.
find <directory>: lists all files in that directory. Can chain with grep to quickly locate a file.

find has configurations which can make the search faster: examples:
-type f searches for objects that are "files"
-user root: filters for option whose owner is root.
-size +20k: filter for files with sizes > 20k

The idea of caching your searches: locate command.
locate
This will work with a local database that stores all information about existing files and folders.
We can update this local database with "sudo updatedb".
After updating, you can notice a significant speed increase in the locating of a file
locate *.txt


find -newermnt: newer than modification time.
So you input -newermnt YYYY-MM-DD to find files newer than that date.

find -type f -name "*.conf"
This uses a wildcard - finds files that ends with .conf

2>/dev/null
Calling it will redirect all error messages to a special device file that discards all
data written to it.
Use this to silence error messages or unwanted outputs


wc -l
Very useful tool. wc stands for word count. Can be used to count number of files
(number of lines) in the output.
-l stands for line.




File descriptors in Linux

Think of them as handles to files.

Unix/Linux: In Unix/Linux, file descriptors are connections managed by the kernel to 
facilitate Input/Output operations with files, devices, and other resources.

Windows: In Windows-based systems, the equivalent concept is known as a file handle,
serving a similar purpose of managing connections for I/O operations.

There are default file descriptors: numbers 0, 1, 2 representing stdin, stdout and stderr

When you see : interpret

stdin: program is reading user input; waiting for input
Typically associated with keyboard strokes.
Note that stdin can be redirected: this means you can re-expect input not from the keyboard
but somewhere else. Example is from another file. Done with redirection operator (>)
UNIX will represent this as file descriptor number 0

stdout: this is the default output stream where programs write their output to.
Typically, stdout is associated with the terminal or console.
Also can be redirected like stdin
UNIX will represent this as file descriptor number 1

stderr: typically where the program's error messages and code will be output from.

Also can be redirected like stdin
UNIX will represent this as file descriptor number 2

Think of stdin, stdout and stderr as water pipes.

A typical use case of silencing errors.
As per above: redirect stderr to /dev/null to silence error messages.
This is useful in some scenarios: if you want to ignore error and proceed; clean up
output; prevent code interruption...

Redirection is >; and we can interpret the direction of it to see from and to the
redirections are directed at.

Double arrows >> implies appending to a file.

You can chain double arrows with End Of File (EOF) function to define an input's end.

Another form of redirection is piping (|).
Typically used with grep; where we use regular expressions to pattern-match and extract
desired found data.




Filtering of contents

more
This tool helps display contents of a file in page sections.
So you can press some other key to go to the next page.

less
Similar to more. Will display the content of a file nicely.

head
This shows you the first 10 lines of file

tail
Similarly, this shows you the last 10 lines of file

sort
We can use this after piping.
| sort
This will sort the input in alphabetical or numerical order

grep
Uses pattern to pattern match and extract out found text.
-v will exclude the results.

To remove delimiters, we can use cut

cut
This is useful to strip out delimiters:
cut -d":" -f1
The -d stands for delimiter. In this case, we are setting it to ":'
Then, -f1 will display the first index after splitting.
So if you have the form:
a:b:c:d:e
and you run cut -d":" -f4, you will display "d" only.
The number is the index to show in that delimiter-split line.

tr
Replaces certain characters from a line with.
Stands for translate.
Usage is: tr "toReplace" "replaced"
Used after piping.

column -t
This is used to display in column-form.
-t is tabular form
Used after piping



awk
A text processing tool. Programming language as well.
It does field-based processing.
So a typical command would be like:
awk '{print $1, $NF}'
Fields are referenced by $1, $2, $3, etc.,representing
the first, second, third, and so on fields in a line.

The special variable $NF refers to the last field in the line.



sed
Stands for stream editor
Can be used for substituting patterns and other commands:
sed looks for patterns we have defined in the form of regular
expressions (regex) and replaces them with another pattern that
we have also defined.
| sed 's/bin/HTB/g'
s flag at the start stands for substitute command.
Then after the first / is the pattern (bin)
Then after the second / is the replacement text (HTB)
g flag at the end stands for global. Signals to replace all matches.



wc
Word count. -l specifies that only lines are counted.

To see packages are installed in Linux system:
apt list --installed.

grep -c "pattern" counts and returns how many times the pattern occurred. 

sort -u returns all unique lines; removes duplicates




Regular expressions

Round brackets: logical grouping
Square brackets: define character list to search for
Curly brackets: specifies range of how many times a pattern should be repeated

If you want to extend grep and use operators like OR or AND, you must specify with
parameter -E
then grep -E "(characterCheck|anotherCharacterCheck)" /etc/passwd

for AND operator: substitute | for .*
For AND operator, the order must be enforced. If the operands are not in the correct
order, there will be no return





Permission management
Linux assigns permissions to users and groups.

Users can be part of a group. If so, they will be given specific permissions.
"Permission Denied" error means the user has no right to access the file.


execute permission
This permission allows for traversal of directories.
It does not, contrary to its name, allow a user to execute or modify any files
or content within a directory.


To change permission (of a file or directory)
You will need write access to the file or directory.
In Linux, there are 3 types of permissions of a file or directory:
(r) - Read
(w) - Write
(x) - Execute
The syntax of what is being displayed and permissions:

- rwx rw- r--   1 root root 1641 May  4 23:42 /etc/passwd
- --- --- ---   |  |    |    |   |__________|
|  |   |   |    |  |    |    |        |_ Date
|  |   |   |    |  |    |    |__________ File Size
|  |   |   |    |  |    |_______________ Group
|  |   |   |    |  |____________________ User
|  |   |   |    |_______________________ Number of hard links
|  |   |   |_ Permission of others (read)
|  |   |_____ Permissions of the group (read, write)
|  |_________ Permissions of the owner (read, write, execute)
|____________ File type (- = File, d = Directory, l = Link, ... )

We use:
chmod <permission group reference><+ or - to add or remove permission><read, write and or execute> <file name>
There are a few permission groups: u - owner, g - Group, o - others, a - All users.

Example:
chmod o+x shell
This sets the access rights of those in group "others" the execute (x) permission on the executable "shell"

A little background on how the permissions are represented in binary:
3 permissions so 000 to 111
Going by position, Linux has r w x representing the positions in the binary string 111.
If the file lacks for example write permission, Linux will substitute the middle 1 with 0.
So we get 101. Then, for each 0, Linux substitutes it with a "-". So we will get r - x



To change owner of a file:
chown <new user>:<new group> <file/directory to change>



Consider the case of having to change each file's permissions to suit a user.
A simpler approach is to change the rights of the user. This can be done by setting the bits
of SUID and SGID.

SUID and SGID
Set User ID and Set Group ID. This enables users to run programs with the rights of another
user or group.



Sticky bits
These are special bits: makes the directory only deletable or renameable by the root or owner.
Useful for shared files.
Represented by letter "t" in directories.
Generally used for directories only.
T in the following means that the directory has no execute permission for "others": see the second.

drw-rw-r-t 3 cry0l1t3 cry0l1t3   4096 Jan 12 12:30 scripts
drw-rw-r-T 3 cry0l1t3 cry0l1t3   4096 Jan 12 12:32 reports

Observe that the "x" position is replaced with t.



User management

su
Switch user.
Change the effective user ID and group ID to that of <user>: su <user>


useradd: adds a new user or updates default new user information
sudo useradd newUsername

userdel: deletes an existing user
sudo userdel newUsername

usermod: modifies an existing user account
sudo usermod -G groupName newUsername
The above will update the user "newUsername" to be in a group called "groupName"

addgroup: adds a new group
sudo addgroup groupname
This creates a new group called "groupname". The name has to be in all lowercase
or as per defined by the regex NAME_REGEX[_SYSTEM] configuration variable.

delgroup: deletes an existing group
sudo delgroup groupname

passwd: changes user's password
sudo passwd newUser
This changes the password for the user "newUser"



Package Management
There are many different types of package management software. Each covers different types of files:
".deb", ".rpm", ...
These are typically created and maintained under Linux distributions.
If the package requires other libraries; it will prompt the user and then install them.
It just manages adding new packages; deleting them and updating of them in a controlled manner.

Popular one is dpkg. dpkg is a tool to install build remove and manage Debian packages.
A front-end for dpkg is aptitude: alternative to apt.

apt and aptitude: high-level command-line interface for package management system

snap enables installing, configuring, refreshing and deleting snap packages. snap packages are secured
distributions of the packages.

gem is another front-end for RubyGems. RubyGems, like dpkg, is a package manager tool, but for Ruby.

pip is another front-end for Python. Meant for Python packages.

git is a distributed revision control system.


Debian-based Linux systems use the APT package manager.
A package will contain multiple ".deb" files: we use dpkg to install these .deb files.
Benefit of APT in this case: if it detects required dependencies, it will install them.

When we update or install a new package, the system will query the software repositories in its 
respective Linux distribution.

APT uses a database called APT cache.
These contains information about the packages offline:
apt-cache search <package>

To search for more information:
apt-cache show <package>



To show all installed packages
apt list --installed




Service and Process Management
In general, there are two kinds of services: internal, which are relevant services required
on system startup, and user services - installed by the user.
Server services run in the background without user interaction.

These are called daemons and are typically represented by a postfix "d" after the program name:
sshd or systemd

systemd exists in most Linux distributions, and is responsible for orderly starting and stopping
of other processes.

systemd's logging service is journalctl.
journalctl prints the log from journald, a service for systemd

All processes have an id: can be viewed under /proc/ with the corresponding number.

We can use systemctl to list all services

To list processes: ps
ps -aux
Lists processes
-aux:
a = show processes for all users
u = display the process's user/owner
x = also show processes not attached to a terminal

A process can be in any of the following states:
Running
Waiting (waiting for an event or system resource)
Stopped
Zombie (stopped but still has an entry in the process table).

We can stop or kill a process: kill, pkill, pgrep and killall.
The idea is to send a signal to stop the process. kill -l will list all signals
The signals each are denoted by a number; the number represents a signal.
We call kill 9 <process id > to send signal 9 to the process

All background jobs can be displayed with "jobs"

To send a process to the background:
run the command => press CTRL Z to suspend the process => "bg" to send to background
Alternative => Set an "&" at the end of the command.

When the job is done, it will appear on console.

Similarly, if you want to bring back a job in the background to the foreground, run "fg"


Execution of multiple commands:
There are 3 approaches: each having different consequences: ; && |

;
Separates commands and ignores previous commands' errors and results

&&
Runs commands one after the other. Terminates on error.

|
Uses the previous result and pipe it into another command.



Task Scheduling in Linux
We can use systemd to:
1. Create a timer
2. Create a service
3. Activate the timer

We can also use Cron. This is a tool to schedule and automate processes.
To do so: we create a script for the Cron daemon to call it at a specific time
To set up: we store the tasks in a file called a crontab. Then tell the daemon when to run the tasks



Network Services

SSH
Secure Shell Protocol. Widely used to transmit data.
In order to even connect to a Linux host with SSH; its server must be running. A popular open-source
SSH server is OpenSSH server.

SSH is a way to establish secure remote connections without being intercepted by third parties.

We can check our SSH status: systemctl status ssh

Some use case: login to another machine perform commands and or file transfer without eavesdropping

Network File System (NFS)
This is a protocol that enables us to store and manage files remotely as if it were local.
Just a way to file transfer over the web

We can check NFS status: systemctl status nfs-kernel-server

All details and settings are in /etc/exports. Transfer speed, encryption, access rights...



Web Servers
Uses HTTP to transfer data.
We use a web server to interact with a machine through an incoming HTTP or HTTPS port

You can host your own web server: Apache Web Server
To change settings (global settings): /etc/apache2/apache2.conf

Another approach is using Python: python3 -m http.server
This starts the web server on TCP port 8000. The directory where you ran the command: you 
will be able to view the files in it.


VPN
Allows us to securely connect to a network as if we were directly in it.
Done via an encrypted tunnel between client and server.

Mainly used by companies to provide employees with secure access to the internal network
without being physically present.
Popular ones are like OpenVPN.

Similarly, you can change its settings: /etc/openvpn/server.conf  


Working with web services
Apache has many other tools. Some of it enables you use appropriate modules to encrypt communication
between the browser and web server, or use the server as a proxy server, or perform manipulation of
HTTP header data and URLs.


cURL is a tool that allows us to file transfer from the shell through protocols like HTTP, HTTPS, FTP,
SFTP, FTPS or SCP.
Can be used for testing and controlling websites remotely as well.

curl <website> returns html

Alternative to cURL: wget. wget enables downloading of files from FTP or HTTP servers directly from the
terminal.



Backup and restore
Linux system offer ways to backup and restore data
Tools: Rsync, Deja Dup, Duplicity

Rsync

Backup local to remote server:
rsync -av /path/to/mydirectory user@backup_server:/path/to/backup/directory

-a: archive (preserve file attributes); -v: verbose

Restoring backup:
rsync -av user@remote_host:/path/to/backup/directory /path/to/mydirectory
This is just the reverse of the backup command.

We can also make the file transfer encrypted + use SSH:
rsync -avz -e ssh /path/to/mydirectory user@backup_server:/path/to/backup/directory
-e: encrypted

To make the backup consistent: use cron to schedule the backups; run the command:

First we create the script:

#!/bin/bash

rsync -avz -e ssh /path/to/mydirectory user@backup_server:/path/to/backup/directory

Then make sure that the file is executable and only you can access

Adding a crontab: example
0 * * * * /path/to/RSYNC_Backup.sh


File system management

Linux supports different file systems: ext2, ext3, ext4, XFS, Brtfs, NTFS
More on file systems:
Each file system has its pros and cons.

NTFS is used by Windows

Best choice depends on the user.

Different OS supports different file systems.
Different Linux distributions support different file systems too.

The Linux file system is based on the Unix file system.
It has a hierarchical structure, top is the inode table.



Inode table
This is an table of information associated with each file and directory on a Linux system.
It contains metadata about the file or directory, such as its permissions, size...
Just think of it as a database of file / folder information.
An inode table is referenced when a file operations are performed: create, read, update, delete.
An inode table stores data in an efficient data structure. It differs based on file systems;
Different Linux distributions may have slight variations on the data structure


Files in Linux are stored in two ways: file or directory.
Linux also supports symbolic links



Symbolic links
These are quickly used to access files that are located at a different part of the file system



Each file and directory needs to be managed in terms of permissions.


Disk management in Linux
Tool is fdisk, enabling CRUD operations on a drive.
It can also be used to display a partition table and able to manage it; view its size and type.
One can partition a drive in Linux; enabling one physical storage space to be split up; each with
its own specific file system.

Common partitioning tools are fdisk, gpart and GParted.


What is mounting?
This is a process.
This is when you assign a drive or partition to a particular directory in Linux.
You need to do this to even be able to access the drive.
When you mount a drive, you integrate it into the file system.

We use the "mount" tool to mount file systems on Linux
/etc/fstab file is used to define the default file systems that are mounted at boot time.

"mount" command will display all mounted file systems.
It shows the device name, file system type, mount point, and options.

sudo mount <from_directory> <to_directory>
Some example use case: mounting a USB to a folder on Linux:
You will be able to access the USB's files and directories in that folder you mounted onto in Linux

To unmount: sudo umount <the_to_directory_of_when_you_mount>
Like windows, if there are running processes that is using the directory, you cannot unmount the device.





Swap space in Linux
This is relevant when available physical memory (RAM) is depleted.

At this point, the kernel transfers inactive pages of memory to the swap space. This frees up space in the RAM
The kernel cannot delete the data in the pages as it may be of use in the future.

The swap space will now have more memory to be used by running processes.

You can create the swap space, determine its size.

The command mkswap makes a swap space in a file.
This is done by initialising a designated file or partition to be used as swap space
and creates this header. The header includes information that identifies the file or partition as a swap
space area.

This identification ensures that the kernel recognizes the designated space as available for swapping out
memory pages when necessary.

, and command swapon activates the swap area.

The size of the swap space is up to you.

When creating a swap space, it is important to ensure that it is placed on a dedicated partition or file,
separate from the rest of the file system.
This helps to prevent fragmentation of the swap space (data becomes scattered everywhere
instead of being in a contiguous block).
This also helps ensure that the system has adequate swap space available when it is needed.

It is also important to ensure that the swap space is encrypted, as sensitive data may be stored in the
swap space temporarily.

Excessive swap space might not be good. Disk access is slower than RAM, so you might not want to swap
unless necessary

If there is still not enough space after the swap: the system will experience performance issues. This is
where your system may freeze or crash

It does have some tools: Linux Out of Memory Killer (OOM Killer)
This tool will review all running processes: kill one or more of them to free up space.
How does it determine which process to kill: based on its own scoring system: oom_score
To view all oom scores of all processes: cat /proc/*/oom_score. Higher means more likely to be killed

Swap space is also used for hibernation; sleep state




Containerisation

This is a process of packaging and running applications in an isolated environment.
The environment is called a container, virtual machine or serverless environment.
These running applications are self-sufficient; they have all the libraries, code, system tools, settings
to run separately.

An example is a web application where you put the frontend, backend and database code
and applications in separate, independent and isolated "places".
Then, there are many benefits here. You can scale which component is more taxed; identify which containers
need more resources; figure out which container is responsible for a point of failure.
You can also then, because all the components are separate, have many other new applications that uses just
the contained database but not the frontend or the backend.
Security is another major benefit; there is no longer a single point of failure with containers.

These make containerisation very useful for long-term and large-scale projects.

Docker is an open-source platform for automating the deployment of applications.

Necessary parts to run a container:
1. Docker engine
2. Docker image: Dockerfile
Docker Hub provides both parts

As i think Docker is a topic that can be explored on its own, i will be writing my learning points in a
dedicated separate file.

Linux has its virtualisation technology that enables multiple isolated Linux systems to run on a single host.
This is called "Linux Containers"

There are some differences from Linux's virtualisation technology from Docker.
It has fewer features than Docker, the containers are tied to the host system and can be harder to manage.

Docker builds on top of LXC, and hence is more user-friendly.

A Dockerfile contains the base image and instructions to build the image.
These images are meant to be portable so that they can be moved around.



Network configuration
Management of network settings; network interfaces.


Network Access Control (NAC) is useful to know for network security
There are many NAC technologies available. Each NAC is a model: a model where
depending on what kind of security measures you want to implement, you would
choose the type of Access Control.

1. Discretionary Access Control (DAC)
    Principles: 
        1. Each object has properties; only visible and modifiable by authorised users.
        2. Each failed attempt enforce additional multi-factor authentication. Or denies access.
        3. User can "transfer ownership" of an object or group of objects to another person.
            The user can also control what kind of ownership rights he wants to give the person.
    Pros:
        1. Flexible (each user can control very specifically what kind of rights an object can hold)
        2. User-friendly (fast and easy to change and act on)
    Cons:
        1. Not actually safe because any user can share anything that they have control over; no system
        2. No system or structure to monitor a user's activity. No centralised access management

2. Mandatory Access Control (MAC) : considered most secure
    Principles: 
        1. Centralised - only the administrator determines who has the rights to view or edit files.
        2. Each object has a confidentiality level
        3. Each user has a clearance level
        4. Using 2, 3, which are provided by 1., the OS either allows or rejects the desire to view or
        edit the file
    Pros:
        1. Secure and fewer errors. Because now only the administrator (one person) determines who gets
        the rights to the file; not just any user.
        2. User-friendly (fast and easy to change and act on)
    Cons:
        1. Not flexible (employees must ask administrator to vet their clearance for a file everytime)
        2. Because of 1., it is not scalable automatically. Each new object or person needs to be vetted 
3. Role-based Access Control (RBAC)
    Principles: 
        1. Permissions are given to users based on their roles
        2. Users are grouped into roles, and permissions are assigned to roles.
    Pros: 
        1. Structurally simple: if you have role X, you get permissions X to do the role.
        2. Because of 1., it is easier to maintain and still preserves security
    Cons: 
        1. Can be complex if there are many roles and permissions. Since each needs careful analysis

Linux enables one to configure NAC to adopt one of these models.


Configuration of Network Interfaces

ifconfig
This enables viewing and editing of system's network interfaces.
Note that it is slowly being replaced with "ip"

Network interface configurations typically use either ifconfig or ip.

You can activate a network interface: sudo ifconfig eth0 up
You can allocate IP to a network interface: sudo ifconfig eth0 <ip address>
Similarly, you can also change the netmask for an interface: sudo ifconfig eth0 netmask 255.255.255.0


Default Gateway
A default gateway is the ip address of the router.
You can change the IP address of it using the "route" command and "add" option:

sudo route add default gw 192.168.1.1 eth0


When we change IP addresses of any network interface, it is important to reflect the changes in the DNS
servers as well: in /etc/resolv.conf.
This file is temporary; the changes are temporary, and are erased on system restart.

To prevent removal of the changes upon system restart: edit a configuration file in /etc/netplan



Network troubleshooting
There are many times where we have network issues. Tools for analysing the network:
1. Ping
    Test connectivity between two devices: ping <ip address>
2. Traceroute
    Traces the route which packets take to reach the <ip address>: traceroute <ip address>.
    How it works: it uses TTL:
        1. Computer sends an ICMP request to an intermediary device with TTL value of 1.
        2. On reaching device, TTL value decreases to 0, and is dropped. It then responds to our
        computer with "TTL exceeded".
        3. Now on computer does the same thing as in 1: send back to the same device, but now with
        TTL value of 2. Because it now does not decrement to 0, it will pass on to the next device.
        4. The next device decrements once more the TTL value to 0. Then it will respond to our
        computer with "TTL exceeded".
        5. Our computer repeats step 1 to 5, with increasing TTL value by 1 each time; until the
        target device OR traceroute hits its maximimum hop count: default is 30.
    Along the process, it tracks all the devices it visited via the response of "TTL exceeded"
    Output shows the
        1. Hop number: Index of the hop
        2. Round-Trip Time: Time taken to send to a device X. And, time taken it took for packet to
        reach our computer
        3. Devices: IP addresses of found devices that responded
3. Netstat
4. Tcpdump
5. Wireshark
6. Nmap